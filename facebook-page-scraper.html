<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Facebook Page Scraper: Scraping Business Data Without API Login</title>
    <meta name="description" content="Notes on scraping Facebook business pages without API or account login. How to extract competitor comments and contact info safely.">
    <meta name="keywords" content="Facebook scraping, business data, competitor analysis, Python, Selenium, marketing">
    <link rel="canonical" href="https://pnt.jacbex.com/facebook-page-scraper.html" />

    <!-- Open Graph / Social Media -->
    <meta property="og:title" content="Facebook Page Scraper - No API Login Required">
    <meta property="og:description" content="How I scrape Facebook business data without API or login. Extract competitor insights safely.">
    <meta property="og:type" content="article">

    <!-- Tailwind CSS for modern UI -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Code Highlighting (Prism.js) -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

    <!-- Structured Data (Schema.org) for Google -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "TechArticle",
      "headline": "Facebook Page Scraper: Scraping Business Data Without API or Account Login",
      "description": "A practical guide to scraping Facebook business pages for competitor analysis without using the official API or logging in.",
      "author": {
        "@type": "Person",
        "name": "Programming Notes"
      },
      "proficiencyLevel": "Intermediate"
    }
    </script>

    <style>
        body { font-family: 'Inter', sans-serif; scroll-behavior: smooth; }
        .prose code { color: #eb5757; background: #f9f2f2; padding: 2px 4px; border-radius: 4px; }
        .ad-placeholder { background: #f3f4f6; border: 2px dashed #d1d5db; text-align: center; padding: 20px; margin: 20px 0; }
    </style>
</head>
<body class="bg-gray-50 text-gray-900">

    <!-- Navigation -->
    <nav class="bg-white shadow-sm sticky top-0 z-50">
        <div class="max-w-4xl mx-auto px-4 py-4 flex justify-between items-center">
            <a href="index.html" class="text-xl font-bold text-blue-600">üìù <span class="text-gray-700">Programming Notes</span></a>
            <div class="space-x-6 hidden md:block">
                <a href="index.html" class="hover:text-blue-500">Home</a>
                <a href="about.html" class="hover:text-blue-500">About Us</a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="bg-gradient-to-r from-blue-700 to-indigo-800 text-white py-16">
        <div class="max-w-4xl mx-auto px-4 text-center">
            <h1 class="text-4xl md:text-5xl font-extrabold mb-4">Facebook Page Scraper: No API or Login</h1>
            <p class="text-xl opacity-90">Pull competitor data from Facebook pages without risking account bans.</p>
        </div>
    </header>

    <main class="max-w-4xl mx-auto px-4 py-12 bg-white mt-[-40px] rounded-xl shadow-lg mb-20">

        <article class="prose prose-blue max-w-none">

            <p class="mb-4">
                Needed to check what competitors were doing on Facebook. Their posts, customer comments, common questions. Facebook's API requires approval and has rate limits. Didn't feel like dealing with any of that.
            </p>
            <p class="mb-4">
                So I wrote a scraper that just loads public pages. No login, no API key. Been using it for a few months now.
            </p>

            <p class="mb-4">
                Facebook pages are public. The issue is the JavaScript rendering and bot detection. Regular Selenium gets blocked immediately. What works is undetected-chromedriver - it's a patched version of Selenium that bypasses detection.
            </p>

            <p class="mb-4">Install this stuff:</p>

            <pre class="language-bash"><code>pip install selenium undetected-chromedriver beautifulsoup4 pandas</code></pre>

            <p class="mb-4 mt-6">You need Chrome installed. The driver downloads automatically.</p>

            <p class="mb-4">
                Watch out for Chrome version. If you update Chrome immediately when a new version drops, undetected-chromedriver might not support it yet. I stay on Chrome 120-125 range.
            </p>

            <p class="mb-4">Here's the scraper:</p>

            <pre class="language-python"><code>import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import pandas as pd
import re

def scrape_facebook_page(page_url, max_scrolls=10):
    options = uc.ChromeOptions()

    # Uncomment for production
    # options.add_argument('--headless')

    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')

    driver = uc.Chrome(options=options)

    try:
        driver.get(page_url)

        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, '[role="feed"]'))
        )

        posts_data = []

        for i in range(max_scrolls):
            soup = BeautifulSoup(driver.page_source, 'html')

            # These selectors change. Check current page structure.
            posts = soup.find_all('div', {'data-pagelet': True})

            for post in posts:
                try:
                    text_elem = post.find('div', {'data-ad-preview': 'message'})
                    text = text_elem.get_text(strip=True) if text_elem else ''

                    reactions = post.find('span', {'class': re.compile(r'.*like.*', re.I)})
                    reactions_count = reactions.get_text(strip=True) if reactions else '0'

                    link_elem = post.find('a', href=re.compile(r'/posts/'))
                    link = 'https://facebook.com' + link_elem['href'] if link_elem else page_url

                    if text and text not in [p['text'] for p in posts_data]:
                        posts_data.append({
                            'text': text,
                            'reactions': reactions_count,
                            'link': link
                        })
                except:
                    continue

            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
            time.sleep(2)

        return pd.DataFrame(posts_data)

    finally:
        driver.quit()</code></pre>

            <p class="mb-4 mt-6">I do 5-10 scrolls normally. Gets a few dozen posts. More than that and you start getting duplicates as the feed refreshes.</p>

            <p class="mb-4">
                The comments are more interesting. People ask about shipping, pricing, problems with orders. Here's how I pull those:
            </p>

            <pre class="language-python"><code>def get_post_comments(post_url):
    driver = uc.Chrome()
    comments_data = []

    try:
        driver.get(post_url)
        time.sleep(3)

        # Expand comments
        for _ in range(3):
            try:
                view_more = driver.find_element(By.XPATH, '//div[contains(text(), "View more comments")]')
                view_more.click()
                time.sleep(2)
            except:
                break

        soup = BeautifulSoup(driver.page_source, 'html')
        comment_blocks = soup.find_all('div', {'aria-label': re.compile(r'Comment', re.I)})

        for block in comment_blocks:
            try:
                author = block.find('span', {'class': re.compile(r'.*name.*', re.I)})
                author_name = author.get_text(strip=True) if author else 'Unknown'

                text = block.find('div', {'data-testid': 'comment_message'})
                comment_text = text.get_text(strip=True) if text else ''

                # Pull out contact info if people leave it
                email_match = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', comment_text)
                phone_match = re.search(r'[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}', comment_text)

                if comment_text:
                    comments_data.append({
                        'author': author_name,
                        'comment': comment_text,
                        'email': email_match.group() if email_match else '',
                        'phone': phone_match.group() if phone_match else ''
                    })
            except:
                continue

        return pd.DataFrame(comments_data)

    finally:
        driver.quit()</code></pre>

            <p class="mb-4 mt-6">If I see a bunch of comments asking about international shipping, I know that's a weak spot for that competitor. Useful stuff.</p>

            <p class="mb-4">The about page usually has email/phone:</p>

            <pre class="language-python"><code>def get_page_contact_info(page_url):
    about_url = page_url.rstrip('/') + '/about'
    driver = uc.Chrome()

    try:
        driver.get(about_url)
        time.sleep(5)

        soup = BeautifulSoup(driver.page_source, 'html')

        contact_info = {'email': '', 'phone': '', 'website': '', 'address': ''}

        page_text = soup.get_text()
        email_match = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', page_text)
        if email_match:
            contact_info['email'] = email_match.group()

        phone_match = re.search(r'[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}', page_text)
        if phone_match:
            contact_info['phone'] = phone_match.group()

        for link in soup.find_all('a', href=True):
            if 'http' in link['href'] and 'facebook' not in link['href']:
                contact_info['website'] = link['href']
                break

        return contact_info

    finally:
        driver.quit()</code></pre>

            <p class="mb-4 mt-6">Run through a list of competitors with delays:</p>

            <pre class="language-python"><code>import random

competitor_pages = [
    'https://www.facebook.com/competitor1',
    'https://www.facebook.com/competitor2',
    'https://www.facebook.com/competitor3',
]

all_data = []

for page in competitor_pages:
    try:
        posts = scrape_facebook_page(page, max_scrolls=5)
        contact = get_page_contact_info(page)

        posts['page_url'] = page
        posts['contact_email'] = contact['email']
        posts['contact_phone'] = contact['phone']

        all_data.append(posts)

        time.sleep(random.uniform(5, 15))

    except Exception as e:
        print(f"Failed on {page}: {e}")
        continue

final_df = pd.concat(all_data, ignore_index=True)
final_df.to_csv('competitor_analysis.csv', index=False)</code></pre>

            <p class="mb-4 mt-6">5-15 seconds between requests works. Less than that and you'll get temp blocked.</p>

            <p class="mb-4">
                Some issues I hit:
            </p>

            <p class="mb-2"><strong>Getting blocked</strong> - If you see captcha, slow down. Try a different user agent or run at off-hours. Facebook's detection varies by time of day.</p>

            <p class="mb-2"><strong>Selectors breaking</strong> - Facebook changes class names constantly. What worked last week might break today. Use role attributes and data attributes when possible.</p>

            <p class="mb-2"><strong>Empty results</strong> - Usually means the page is private, JS didn't finish loading, or content is geo-blocked.</p>

            <p class="mb-2"><strong>Memory</strong> - Chrome leaks memory. Restart browser every 20-30 pages.</p>

            <p class="mb-4 mt-6">
                This works for light scraping. If you need thousands of pages regularly, look into proxies and rotating user agents.
            </p>

            <p class="mb-4">
                I save everything to a database and run weekly scans. Helps track how competitor content changes over time.
            </p>

            <p class="mb-4">
                Haven't figured out geo-location handling yet - Facebook shows different content based on IP. Let me know if you have a cheap solution for this.
            </p>

            <p class="mb-4">
                Legal note: only scrape public pages. Private profiles and groups are against Facebook's terms.
            </p>

        </article>

    </main>

    <footer class="bg-gray-800 text-white py-12">
        <div class="max-w-4xl mx-auto px-4 text-center">
            <p class="opacity-70 mb-2">üìù Programming Notes</p>
            <p class="text-sm opacity-50">
                Personal learning notes & tutorials. Updated as I explore new technologies.
            </p>
        </div>
    </footer>

    <!-- Code Highlighting Script -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
</body>
</html>